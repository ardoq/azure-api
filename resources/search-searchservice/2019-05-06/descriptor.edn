{:scheme "https", :host "azure.local", :info {:description "Client that can be used to manage and query indexes and documents, as well as manage other resources, on a search service.", :title "SearchServiceClient", :version "2019-05-06"}, :ops {:Indexers_List {:path "/indexers", :description "Lists all indexers available for a search service.", :parameters [#:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :DataSources_List {:path "/datasources", :description "Lists all datasources available for a search service.", :parameters [#:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :Indexes_List {:path "/indexes", :description "Lists all indexes available for a search service.", :parameters [{:description "Selects which properties of the index definitions to retrieve. Specified as a comma-separated list of JSON property names, or '*' for all properties. The default is all properties.", :in "query", :name "$select", :required false, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :Skillsets_Get {:path "/skillsets('{skillsetName}')", :description "Retrieves a skillset in a search service.", :parameters [{:description "The name of the skillset to retrieve.", :in "path", :name "skillsetName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :DataSources_CreateOrUpdate {:path "/datasources('{dataSourceName}')", :description "Creates a new datasource or updates a datasource if it already exists.", :parameters [{:description "The name of the datasource to create or update.", :in "path", :name "dataSourceName", :required true, :type "string"} {:description "The definition of the datasource to create or update.", :in "body", :name "dataSource", :required true, :schema #:reference{:definitions "DataSource"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "PreferHeaderParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :put}, :SynonymMaps_CreateOrUpdate {:path "/synonymmaps('{synonymMapName}')", :description "Creates a new synonym map or updates a synonym map if it already exists.", :parameters [{:description "The name of the synonym map to create or update.", :in "path", :name "synonymMapName", :required true, :type "string"} {:description "The definition of the synonym map to create or update.", :in "body", :name "synonymMap", :required true, :schema #:reference{:definitions "SynonymMap"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "PreferHeaderParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :put}, :Indexers_Create {:path "/indexers", :description "Creates a new indexer.", :parameters [{:description "The definition of the indexer to create.", :in "body", :name "indexer", :required true, :schema #:reference{:definitions "Indexer"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :Indexes_CreateOrUpdate {:path "/indexes('{indexName}')", :description "Creates a new search index or updates an index if it already exists.", :parameters [{:description "The definition of the index to create or update.", :in "path", :name "indexName", :required true, :type "string"} {:description "The definition of the index to create or update.", :in "body", :name "index", :required true, :schema #:reference{:definitions "Index"}} {:description "Allows new analyzers, tokenizers, token filters, or char filters to be added to an index by taking the index offline for at least a few seconds. This temporarily causes indexing and query requests to fail. Performance and write availability of the index can be impaired for several minutes after the index is updated, or longer for very large indexes.", :in "query", :name "allowIndexDowntime", :required false, :type "boolean"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "PreferHeaderParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :put}, :Indexers_Get {:path "/indexers('{indexerName}')", :description "Retrieves an indexer definition.", :parameters [{:description "The name of the indexer to retrieve.", :in "path", :name "indexerName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :Indexers_Run {:path "/indexers('{indexerName}')/search.run", :description "Runs an indexer on-demand.", :parameters [{:description "The name of the indexer to run.", :in "path", :name "indexerName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :Indexers_Reset {:path "/indexers('{indexerName}')/search.reset", :description "Resets the change tracking state associated with an indexer.", :parameters [{:description "The name of the indexer to reset.", :in "path", :name "indexerName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :DataSources_Delete {:path "/datasources('{dataSourceName}')", :description "Deletes a datasource.", :parameters [{:description "The name of the datasource to delete.", :in "path", :name "dataSourceName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :delete}, :Indexers_Delete {:path "/indexers('{indexerName}')", :description "Deletes an indexer.", :parameters [{:description "The name of the indexer to delete.", :in "path", :name "indexerName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :delete}, :Indexes_Create {:path "/indexes", :description "Creates a new search index.", :parameters [{:description "The definition of the index to create.", :in "body", :name "index", :required true, :schema #:reference{:definitions "Index"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :GetServiceStatistics {:path "/servicestats", :description "Gets service level statistics for a search service.", :parameters [#:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :SynonymMaps_Create {:path "/synonymmaps", :description "Creates a new synonym map.", :parameters [{:description "The definition of the synonym map to create.", :in "body", :name "synonymMap", :required true, :schema #:reference{:definitions "SynonymMap"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :Indexes_Analyze {:path "/indexes('{indexName}')/search.analyze", :description "Shows how an analyzer breaks text into tokens.", :parameters [{:description "The name of the index for which to test an analyzer.", :in "path", :name "indexName", :required true, :type "string"} {:description "The text and analyzer or analysis components to test.", :in "body", :name "request", :required true, :schema #:reference{:definitions "AnalyzeRequest"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :Skillsets_CreateOrUpdate {:path "/skillsets('{skillsetName}')", :description "Creates a new skillset in a search service or updates the skillset if it already exists.", :parameters [{:description "The name of the skillset to create or update.", :in "path", :name "skillsetName", :required true, :type "string"} {:description "The skillset containing one or more skills to create or update in a search service.", :in "body", :name "skillset", :required true, :schema #:reference{:definitions "Skillset"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "PreferHeaderParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :put}, :Skillsets_List {:path "/skillsets", :description "List all skillsets in a search service.", :parameters [#:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :Indexes_Get {:path "/indexes('{indexName}')", :description "Retrieves an index definition.", :parameters [{:description "The name of the index to retrieve.", :in "path", :name "indexName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :SynonymMaps_List {:path "/synonymmaps", :description "Lists all synonym maps available for a search service.", :parameters [#:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :Indexes_Delete {:path "/indexes('{indexName}')", :description "Deletes a search index and all the documents it contains.", :parameters [{:description "The name of the index to delete.", :in "path", :name "indexName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :delete}, :Skillsets_Create {:path "/skillsets", :description "Creates a new skillset in a search service.", :parameters [{:description "The skillset containing one or more skills to create in a search service.", :in "body", :name "skillset", :required true, :schema #:reference{:definitions "Skillset"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :DataSources_Create {:path "/datasources", :description "Creates a new datasource.", :parameters [{:description "The definition of the datasource to create.", :in "body", :name "dataSource", :required true, :schema #:reference{:definitions "DataSource"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :post}, :Skillsets_Delete {:path "/skillsets('{skillsetName}')", :description "Deletes a skillset in a search service.", :parameters [{:description "The name of the skillset to delete.", :in "path", :name "skillsetName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :delete}, :SynonymMaps_Delete {:path "/synonymmaps('{synonymMapName}')", :description "Deletes a synonym map.", :parameters [{:description "The name of the synonym map to delete.", :in "path", :name "synonymMapName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :delete}, :Indexers_CreateOrUpdate {:path "/indexers('{indexerName}')", :description "Creates a new indexer or updates an indexer if it already exists.", :parameters [{:description "The name of the indexer to create or update.", :in "path", :name "indexerName", :required true, :type "string"} {:description "The definition of the indexer to create or update.", :in "body", :name "indexer", :required true, :schema #:reference{:definitions "Indexer"}} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "IfMatchParameter"} #:reference{:parameters "IfNoneMatchParameter"} #:reference{:parameters "PreferHeaderParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :put}, :Indexers_GetStatus {:path "/indexers('{indexerName}')/search.status", :description "Returns the current status and execution history of an indexer.", :parameters [{:description "The name of the indexer for which to retrieve status.", :in "path", :name "indexerName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :SynonymMaps_Get {:path "/synonymmaps('{synonymMapName}')", :description "Retrieves a synonym map definition.", :parameters [{:description "The name of the synonym map to retrieve.", :in "path", :name "synonymMapName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :DataSources_Get {:path "/datasources('{dataSourceName}')", :description "Retrieves a datasource definition.", :parameters [{:description "The name of the datasource to retrieve.", :in "path", :name "dataSourceName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}, :Indexes_GetStatistics {:path "/indexes('{indexName}')/search.stats", :description "Returns statistics for the given index, including a document count and storage usage.", :parameters [{:description "The name of the index for which to retrieve statistics.", :in "path", :name "indexName", :required true, :type "string"} #:reference{:parameters "ClientRequestIdParameter"} #:reference{:parameters "ApiVersionParameter"}], :verb :get}}, :parameters {:ApiVersionParameter {:description "Client Api Version.", :in "query", :name "api-version", :required true, :type "string"}, :ClientRequestIdParameter {:description "The tracking ID sent with the request to help with debugging.", :format "uuid", :name "client-request-id", :type "string", :x-ms-client-request-id true, :x-ms-parameter-location "method", :x-ms-parameter-grouping {:name "search-request-options"}, :required false, :in "header"}, :IfMatchParameter {:description "Defines the If-Match condition. The operation will be performed only if the ETag on the server matches this value.", :in "header", :name "If-Match", :required false, :type "string", :x-ms-parameter-grouping {:name "access-condition"}, :x-ms-parameter-location "method"}, :IfNoneMatchParameter {:description "Defines the If-None-Match condition. The operation will be performed only if the ETag on the server does not match this value.", :in "header", :name "If-None-Match", :required false, :type "string", :x-ms-parameter-grouping {:name "access-condition"}, :x-ms-parameter-location "method"}, :PreferHeaderParameter {:description "For HTTP PUT requests, instructs the service to return the created/updated resource on success.", :enum ["return=representation"], :in "header", :name "Prefer", :required true, :type "string", :x-ms-parameter-location "method"}, :SearchDnsSuffixParameter {:default "search.windows.net", :description "The DNS suffix of the search service. The default is search.windows.net.", :in "path", :name "searchDnsSuffix", :required true, :type "string", :x-ms-parameter-location "client", :x-ms-skip-url-encoding true}, :SearchServiceNameParameter {:description "The name of the search service.", :in "path", :name "searchServiceName", :required true, :type "string", :x-ms-parameter-location "client", :x-ms-skip-url-encoding true}}, :definitions {:Skill {:description "Abstract base class for skills.", :discriminator "@odata.type", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-predefined-skills"}, :properties {"@odata.type" {:type "string"}, :context {:description "Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.", :type "string"}, :description {:description "The description of the skill which describes the inputs, outputs, and usage of the skill.", :type "string"}, :inputs {:description "Inputs of the skills could be a column in the source data set, or the output of an upstream skill.", :items #:reference{:definitions "InputFieldMappingEntry"}, :type "array"}, :name {:description "The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.", :type "string"}, :outputs {:description "The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.", :items #:reference{:definitions "OutputFieldMappingEntry"}, :type "array"}}, :required ["@odata.type" "inputs" "outputs"]}, :CognitiveServices {:description "Abstract base class for describing any cognitive service resource attached to the skillset.", :discriminator "@odata.type", :properties {"@odata.type" {:type "string"}, :description {:type "string"}}, :required ["@odata.type"]}, :WordDelimiterTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Splits words into subwords and performs optional transformations on subword groups. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.html"}, :properties {:preserveOriginal {:default false, :description "A value indicating whether original words will be preserved and added to the subword list. Default is false.", :type "boolean"}, :catenateWords {:default false, :description "A value indicating whether maximum runs of word parts will be catenated. For example, if this is set to true, \"Azure-Search\" becomes \"AzureSearch\". Default is false.", :type "boolean"}, :generateWordParts {:default true, :description "A value indicating whether to generate part words. If set, causes parts of words to be generated; for example \"AzureSearch\" becomes \"Azure\" \"Search\". Default is true.", :type "boolean"}, :protectedWords {:description "A list of tokens to protect from being delimited.", :items {:type "string"}, :type "array"}, :catenateAll {:default false, :description "A value indicating whether all subword parts will be catenated. For example, if this is set to true, \"Azure-Search-1\" becomes \"AzureSearch1\". Default is false.", :type "boolean"}, :catenateNumbers {:default false, :description "A value indicating whether maximum runs of number parts will be catenated. For example, if this is set to true, \"1-2\" becomes \"12\". Default is false.", :type "boolean"}, :generateNumberParts {:default true, :description "A value indicating whether to generate number subwords. Default is true.", :type "boolean"}, :stemEnglishPossessive {:default true, :description "A value indicating whether to remove trailing \"'s\" for each subword. Default is true.", :type "boolean"}, :splitOnCaseChange {:default true, :description "A value indicating whether to split words on caseChange. For example, if this is set to true, \"AzureSearch\" becomes \"Azure\" \"Search\". Default is true.", :type "boolean"}, :splitOnNumerics {:default true, :description "A value indicating whether to split on numbers. For example, if this is set to true, \"Azure1Search\" becomes \"Azure\" \"1\" \"Search\". Default is true.", :type "boolean"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.WordDelimiterTokenFilter"}, :ServiceCounters {:description "Represents service-level resource counters and quotas.", :properties {:dataSourcesCount #:reference{:definitions "ResourceCounter"}, :documentCount #:reference{:definitions "ResourceCounter"}, :indexersCount #:reference{:definitions "ResourceCounter"}, :indexesCount #:reference{:definitions "ResourceCounter"}, :storageSize #:reference{:definitions "ResourceCounter"}, :synonymMaps #:reference{:definitions "ResourceCounter"}}}, :OcrSkillLanguage {:description "The language codes supported for input by OcrSkill.", :enum ["zh-Hans" "zh-Hant" "cs" "da" "nl" "en" "fi" "fr" "de" "el" "hu" "it" "ja" "ko" "nb" "pl" "pt" "ru" "es" "sv" "tr" "ar" "ro" "sr-Cyrl" "sr-Latn" "sk"], :type "string", :x-ms-enum {:modelAsString false, :name "OcrSkillLanguage"}}, :EdgeNGramTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html"}, :properties {:maxGram {:default 2, :description "The maximum n-gram length. Default is 2. Maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :minGram {:default 1, :description "The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.", :format "int32", :maximum 300, :type "integer"}, :tokenChars {:description "Character classes to keep in the tokens.", :items #:reference{:definitions "TokenCharacterKind"}, :type "array"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.EdgeNGramTokenizer"}, :CommonGramTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed too, with bigrams overlaid. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html"}, :properties {:commonWords {:description "The set of common words.", :items {:type "string"}, :type "array"}, :ignoreCase {:default false, :description "A value indicating whether common words matching will be case insensitive. Default is false.", :type "boolean"}, :queryMode {:default false, :description "A value that indicates whether the token filter is in query mode. When in query mode, the token filter generates bigrams and then removes common words and single terms followed by a common word. Default is false.", :type "boolean", :x-ms-client-name "UseQueryMode"}}, :required ["commonWords"], :x-ms-discriminator-value "#Microsoft.Azure.Search.CommonGramTokenFilter"}, :IndexListResult {:description "Response from a List Indexes request. If successful, it includes the full definitions of all indexes.", :properties {:value {:description "The indexes in the Search service.", :items #:reference{:definitions "Index"}, :readOnly true, :type "array", :x-ms-client-name "Indexes"}}}, :DictionaryDecompounderTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Decomposes compound words found in many Germanic languages. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.html"}, :properties {:maxSubwordSize {:default 15, :description "The maximum subword size. Only subwords shorter than this are outputted. Default is 15. Maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :minSubwordSize {:default 2, :description "The minimum subword size. Only subwords longer than this are outputted. Default is 2. Maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :minWordSize {:default 5, :description "The minimum word size. Only words longer than this get processed. Default is 5. Maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :onlyLongestMatch {:default false, :description "A value indicating whether to add only the longest matching subword to the output. Default is false.", :type "boolean"}, :wordList {:description "The list of words to match against.", :items {:type "string"}, :type "array"}}, :required ["wordList"], :x-ms-discriminator-value "#Microsoft.Azure.Search.DictionaryDecompounderTokenFilter"}, :HighWaterMarkChangeDetectionPolicy {:allOf [#:reference{:definitions "DataChangeDetectionPolicy"}], :description "Defines a data change detection policy that captures changes based on the value of a high water mark column.", :properties {:highWaterMarkColumnName {:description "The name of the high water mark column.", :type "string"}}, :required ["highWaterMarkColumnName"], :x-ms-discriminator-value "#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy"}, :LengthTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Removes words that are too long or too short. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html"}, :properties {:max {:default 300, :description "The maximum length in characters. Default and maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :min {:default 0, :description "The minimum length in characters. Default is 0. Maximum is 300. Must be less than the value of max.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.LengthTokenFilter"}, :CustomAnalyzer {:allOf [#:reference{:definitions "Analyzer"}], :description "Allows you to take control over the process of converting text into indexable/searchable tokens. It's a user-defined configuration consisting of a single predefined tokenizer and one or more filters. The tokenizer is responsible for breaking text into tokens, and the filters for modifying tokens emitted by the tokenizer.", :properties {:charFilters {:description "A list of character filters used to prepare input text before it is processed by the tokenizer. For instance, they can replace certain characters or symbols. The filters are run in the order in which they are listed.", :items #:reference{:definitions "CharFilterName"}, :type "array"}, :tokenFilters {:description "A list of token filters used to filter out or modify the tokens generated by a tokenizer. For example, you can specify a lowercase filter that converts all characters to lowercase. The filters are run in the order in which they are listed.", :items #:reference{:definitions "TokenFilterName"}, :type "array"}, :tokenizer #:reference{:definitions "TokenizerName"}}, :required ["tokenizer"], :x-ms-discriminator-value "#Microsoft.Azure.Search.CustomAnalyzer"}, :IndexerExecutionStatus {:description "Represents the status of an individual indexer execution.", :enum ["transientFailure" "success" "inProgress" "reset"], :type "string", :x-ms-enum {:name "IndexerExecutionStatus"}, :x-nullable false}, :SplitSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill to split a string into chunks of text.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-textsplit"}, :properties {:defaultLanguageCode #:reference{:definitions "SplitSkillLanguage"}, :maximumPageLength {:description "The desired maximum page length. Default is 10000.", :format "int32", :type "integer", :x-nullable true}, :textSplitMode #:reference{:definitions "TextSplitMode"}}, :x-ms-discriminator-value "#Microsoft.Skills.Text.SplitSkill"}, :LanguageDetectionSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill that detects the language of input text and reports a single language code for every document submitted on the request. The language code is paired with a score indicating the confidence of the analysis.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-language-detection"}, :x-ms-discriminator-value "#Microsoft.Skills.Text.LanguageDetectionSkill"}, :Suggester {:description "Defines how the Suggest API should apply to a group of fields in the index.", :properties {:name {:description "The name of the suggester.", :type "string"}, :searchMode {:description "A value indicating the capabilities of the suggester.", :enum ["analyzingInfixMatching"], :type "string", :x-ms-enum {:modelAsString false, :name "searchMode"}}, :sourceFields {:description "The list of field names to which the suggester applies. Each field must be searchable.", :items {:type "string"}, :type "array"}}, :required ["name" "searchMode" "sourceFields"]}, :EdgeNGramTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Generates n-grams of the given size(s) starting from the front or the back of an input token. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html"}, :properties {:maxGram {:default 2, :description "The maximum n-gram length. Default is 2.", :format "int32", :type "integer"}, :minGram {:default 1, :description "The minimum n-gram length. Default is 1. Must be less than the value of maxGram.", :format "int32", :type "integer"}, :side #:reference{:definitions "EdgeNGramTokenFilterSide"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.EdgeNGramTokenFilter", :x-ms-external true}, :FreshnessScoringFunction {:allOf [#:reference{:definitions "ScoringFunction"}], :description "Defines a function that boosts scores based on the value of a date-time field.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Add-scoring-profiles-to-a-search-index"}, :properties {:freshness #:reference{:definitions "FreshnessScoringParameters"}}, :required ["freshness"], :x-ms-discriminator-value "freshness"}, :ShingleTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Creates combinations of tokens as a single token. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html"}, :properties {:filterToken {:default "_", :description "The string to insert for each position at which there is no token. Default is an underscore (\"_\").", :type "string"}, :maxShingleSize {:default 2, :description "The maximum shingle size. Default and minimum value is 2.", :format "int32", :minimum 2, :type "integer"}, :minShingleSize {:default 2, :description "The minimum shingle size. Default and minimum value is 2. Must be less than the value of maxShingleSize.", :format "int32", :minimum 2, :type "integer"}, :outputUnigrams {:default true, :description "A value indicating whether the output stream will contain the input tokens (unigrams) as well as shingles. Default is true.", :type "boolean"}, :outputUnigramsIfNoShingles {:default false, :description "A value indicating whether to output unigrams for those times when no shingles are available. This property takes precedence when outputUnigrams is set to false. Default is false.", :type "boolean"}, :tokenSeparator {:default " ", :description "The string to use when joining adjacent tokens to form a shingle. Default is a single space (\" \").", :type "string"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.ShingleTokenFilter"}, :TagScoringFunction {:allOf [#:reference{:definitions "ScoringFunction"}], :description "Defines a function that boosts scores of documents with string values matching a given list of tags.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Add-scoring-profiles-to-a-search-index"}, :properties {:tag #:reference{:definitions "TagScoringParameters"}}, :required ["tag"], :x-ms-discriminator-value "tag"}, :OutputFieldMappingEntry {:description "Output field mapping for a skill.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/naming-rules"}, :properties {:name {:description "The name of the output defined by the skill.", :type "string"}, :targetName {:description "The target name of the output. It is optional and default to name.", :type "string"}}, :required ["name"]}, :AsciiFoldingTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127 ASCII characters (the \"Basic Latin\" Unicode block) into their ASCII equivalents, if such equivalents exist. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html"}, :properties {:preserveOriginal {:default false, :description "A value indicating whether the original token will be kept. Default is false.", :type "boolean"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.AsciiFoldingTokenFilter"}, :ImageAnalysisSkillLanguage {:description "The language codes supported for input by ImageAnalysisSkill.", :enum ["en" "zh"], :type "string", :x-ms-enum {:modelAsString false, :name "ImageAnalysisSkillLanguage"}}, :NamedEntityRecognitionSkill {:allOf [#:reference{:definitions "Skill"}], :description "Text analytics named entity recognition. This skill is deprecated in favor of EntityRecognitionSkill.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-named-entity-recognition"}, :properties {:categories {:description "A list of named entity categories.", :items #:reference{:definitions "NamedEntityCategory"}, :type "array"}, :defaultLanguageCode #:reference{:definitions "NamedEntityRecognitionSkillLanguage"}, :minimumPrecision {:description "A value between 0 and 1 to indicate the confidence of the results.", :format "double", :type "number", :x-nullable true}}, :x-ms-discriminator-value "#Microsoft.Skills.Text.NamedEntityRecognitionSkill", :x-ms-external true}, :TextSplitMode {:description "A value indicating which split mode to perform.", :enum ["pages" "sentences"], :type "string", :x-ms-enum {:modelAsString false, :name "TextSplitMode"}}, :DataSourceCredentials {:description "Represents credentials that can be used to connect to a datasource.", :properties {:connectionString {:description "The connection string for the datasource.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Create-Data-Source"}, :type "string"}}}, :KeywordTokenizerV2 {:allOf [#:reference{:definitions "Tokenizer"}], :description "Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html"}, :properties {:maxTokenLength {:default 256, :description "The maximum token length. Default is 256. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.KeywordTokenizerV2"}, :SynonymMap {:description "Represents a synonym map definition.", :properties {"@odata.etag" {:description "The ETag of the synonym map.", :type "string", :x-ms-client-name "ETag"}, :format {:description "The format of the synonym map. Only the 'solr' format is currently supported.", :enum ["solr"], :type "string", :x-ms-enum {:modelAsString false, :name "SynonymMapFormat"}}, :name {:description "The name of the synonym map.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Naming-rules"}, :type "string"}, :synonyms {:description "A series of synonym rules in the specified synonym map format. The rules must be separated by newlines.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Create-Synonym-Map#SynonymMapFormat"}, :type "string"}}, :required ["name" "format" "synonyms"]}, :Skillset {:description "A list of skills.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-tutorial-blob"}, :properties {"@odata.etag" {:description "The ETag of the skillset.", :type "string", :x-ms-client-name "ETag"}, :cognitiveServices #:reference{:definitions "CognitiveServices"}, :description {:description "The description of the skillset.", :type "string"}, :name {:description "The name of the skillset.", :type "string"}, :skills {:description "A list of skills in the skillset.", :items #:reference{:definitions "Skill"}, :type "array"}}, :required ["name" "description" "skills"]}, :NamedEntityRecognitionSkillLanguage {:description "The language codes supported for input text by NamedEntityRecognitionSkill.", :enum ["ar" "cs" "da" "de" "en" "es" "fi" "fr" "he" "hu" "it" "ko" "pt-br" "pt"], :type "string", :x-ms-enum {:modelAsString false, :name "NamedEntityRecognitionSkillLanguage"}}, :DistanceScoringParameters {:description "Provides parameter values to a distance scoring function.", :properties {:boostingDistance {:description "The distance in kilometers from the reference location where the boosting range ends.", :format "double", :type "number"}, :referencePointParameter {:description "The name of the parameter passed in search queries to specify the reference location.", :type "string"}}, :required ["referencePointParameter" "boostingDistance"]}, :TagScoringParameters {:description "Provides parameter values to a tag scoring function.", :properties {:tagsParameter {:description "The name of the parameter passed in search queries to specify the list of tags to compare against the target field.", :type "string"}}, :required ["tagsParameter"]}, :WebApiSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill that can call a Web API endpoint, allowing you to extend a skillset by having it call your custom code.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-custom-skill-web-api"}, :properties {:batchSize {:description "The desired batch size which indicates number of documents.", :format "int32", :type "integer", :x-nullable true}, :degreeOfParallelism {:description "If set, the number of parallel calls that can be made to the Web API.", :format "int32", :type "integer", :x-nullable true}, :httpHeaders #:reference{:definitions "WebApiHttpHeaders"}, :httpMethod {:description "The method for the http request.", :type "string"}, :timeout {:description "The desired timeout for the request. Default is 30 seconds.", :format "duration", :type "string"}, :uri {:description "The url for the Web API.", :type "string"}}, :required ["uri"], :x-ms-discriminator-value "#Microsoft.Skills.Custom.WebApiSkill"}, :Field {:description "Represents a field in an index definition, which describes the name, data type, and search behavior of a field.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Create-Index"}, :properties {:indexAnalyzer #:reference{:definitions "AnalyzerName"}, :key {:description "A value indicating whether the field uniquely identifies documents in the index. Exactly one top-level field in each index must be chosen as the key field and it must be of type Edm.String. Key fields can be used to look up documents directly and update or delete specific documents. Default is false for simple fields and null for complex fields.", :type "boolean"}, :name {:description "The name of the field, which must be unique within the fields collection of the index or parent field.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Naming-rules"}, :type "string"}, :fields {:description "A list of sub-fields if this is a field of type Edm.ComplexType or Collection(Edm.ComplexType). Must be null or empty for simple fields.", :items #:reference{:definitions "Field"}, :type "array"}, :filterable {:description "A value indicating whether to enable the field to be referenced in $filter queries. filterable differs from searchable in how strings are handled. Fields of type Edm.String or Collection(Edm.String) that are filterable do not undergo word-breaking, so comparisons are for exact matches only. For example, if you set such a field f to \"sunny day\", $filter=f eq 'sunny' will find no matches, but $filter=f eq 'sunny day' will. This property must be null for complex fields. Default is true for simple fields and null for complex fields.", :type "boolean"}, :searchAnalyzer #:reference{:definitions "AnalyzerName"}, :type #:reference{:definitions "DataType"}, :searchable {:description "A value indicating whether the field is full-text searchable. This means it will undergo analysis such as word-breaking during indexing. If you set a searchable field to a value like \"sunny day\", internally it will be split into the individual tokens \"sunny\" and \"day\". This enables full-text searches for these terms. Fields of type Edm.String or Collection(Edm.String) are searchable by default. This property must be false for simple fields of other non-string data types, and it must be null for complex fields. Note: searchable fields consume extra space in your index since Azure Cognitive Search will store an additional tokenized version of the field value for full-text searches. If you want to save space in your index and you don't need a field to be included in searches, set searchable to false.", :type "boolean"}, :facetable {:description "A value indicating whether to enable the field to be referenced in facet queries. Typically used in a presentation of search results that includes hit count by category (for example, search for digital cameras and see hits by brand, by megapixels, by price, and so on). This property must be null for complex fields. Fields of type Edm.GeographyPoint or Collection(Edm.GeographyPoint) cannot be facetable. Default is true for all other simple fields.", :type "boolean"}, :sortable {:description "A value indicating whether to enable the field to be referenced in $orderby expressions. By default Azure Cognitive Search sorts results by score, but in many experiences users will want to sort by fields in the documents. A simple field can be sortable only if it is single-valued (it has a single value in the scope of the parent document). Simple collection fields cannot be sortable, since they are multi-valued. Simple sub-fields of complex collections are also multi-valued, and therefore cannot be sortable. This is true whether it's an immediate parent field, or an ancestor field, that's the complex collection. Complex fields cannot be sortable and the sortable property must be null for such fields. The default for sortable is true for single-valued simple fields, false for multi-valued simple fields, and null for complex fields.", :type "boolean"}, :retrievable {:description "A value indicating whether the field can be returned in a search result. You can disable this option if you want to use a field (for example, margin) as a filter, sorting, or scoring mechanism but do not want the field to be visible to the end user. This property must be true for key fields, and it must be null for complex fields. This property can be changed on existing fields. Enabling this property does not cause any increase in index storage requirements. Default is true for simple fields and null for complex fields.", :type "boolean"}, :analyzer #:reference{:definitions "AnalyzerName"}, :synonymMaps {:description "A list of the names of synonym maps to associate with this field. This option can be used only with searchable fields. Currently only one synonym map per field is supported. Assigning a synonym map to a field ensures that query terms targeting that field are expanded at query-time using the rules in the synonym map. This attribute can be changed on existing fields. Must be null or an empty collection for complex fields.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Synonym-Map-operations"}, :items {:type "string"}, :type "array"}}, :required ["name" "type"]}, :MicrosoftLanguageStemmingTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Divides text using language-specific rules and reduces words to their base forms.", :properties {:isSearchTokenizer {:default false, :description "A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. Default is false.", :type "boolean"}, :language #:reference{:definitions "MicrosoftStemmingTokenizerLanguage"}, :maxTokenLength {:default 255, :description "The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300 characters. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the max token length set. Default is 255.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer"}, :FieldMappingFunction {:description "Represents a function that transforms a value from a data source before indexing.", :externalDocs {:url "https://docs.microsoft.com/azure/search/search-indexer-field-mappings"}, :properties {:name {:description "The name of the field mapping function.", :type "string"}, :parameters {:additionalProperties {:type "object"}, :description "A dictionary of parameter name/value pairs to pass to the function. Each value must be of a primitive type.", :type "object"}}, :required ["name"]}, :KeyPhraseExtractionSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill that uses text analytics for key phrase extraction.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-keyphrases"}, :properties {:defaultLanguageCode #:reference{:definitions "KeyPhraseExtractionSkillLanguage"}, :maxKeyPhraseCount {:description "A number indicating how many key phrases to return. If absent, all identified key phrases will be returned.", :format "int32", :type "integer", :x-nullable true}}, :x-ms-discriminator-value "#Microsoft.Skills.Text.KeyPhraseExtractionSkill"}, :Indexer {:description "Represents an indexer.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Indexer-operations"}, :properties {:description {:description "The description of the indexer.", :type "string"}, :schedule #:reference{:definitions "IndexingSchedule"}, :disabled {:default false, :description "A value indicating whether the indexer is disabled. Default is false.", :type "boolean", :x-ms-client-name "IsDisabled"}, :name {:description "The name of the indexer.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Naming-rules"}, :type "string"}, "@odata.etag" {:description "The ETag of the Indexer.", :type "string", :x-ms-client-name "ETag"}, :fieldMappings {:description "Defines mappings between fields in the data source and corresponding target fields in the index.", :externalDocs {:url "https://docs.microsoft.com/azure/search/search-indexer-field-mappings"}, :items #:reference{:definitions "FieldMapping"}, :type "array"}, :dataSourceName {:description "The name of the datasource from which this indexer reads data.", :type "string"}, :targetIndexName {:description "The name of the index to which this indexer writes data.", :type "string"}, :skillsetName {:description "The name of the skillset executing with this indexer.", :type "string"}, :outputFieldMappings {:description "Output field mappings are applied after enrichment and immediately before indexing.", :externalDocs {:url "https://docs.microsoft.com/azure/search/search-indexer-field-mappings"}, :items #:reference{:definitions "FieldMapping"}, :type "array"}, :parameters #:reference{:definitions "IndexingParameters"}}, :required ["name" "dataSourceName" "targetIndexName"]}, :TokenCharacterKind {:description "Represents classes of characters on which a token filter can operate.", :enum ["letter" "digit" "whitespace" "punctuation" "symbol"], :type "string", :x-ms-enum {:modelAsString false, :name "TokenCharacterKind"}}, :DataContainer {:description "Represents information about the entity (such as Azure SQL table or DocumentDb collection) that will be indexed.", :properties {:name {:description "The name of the table or view (for Azure SQL data source) or collection (for DocumentDB data source) that will be indexed.", :type "string"}, :query {:description "A query that is applied to this data container. The syntax and meaning of this parameter is datasource-specific. Not supported by Azure SQL datasources.", :type "string"}}, :required ["name"]}, :ScoringFunctionAggregation {:description "Defines the aggregation function used to combine the results of all the scoring functions in a scoring profile.", :enum ["sum" "average" "minimum" "maximum" "firstMatching"], :type "string", :x-ms-enum {:name "ScoringFunctionAggregation"}}, :MergeSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill for merging two or more strings into a single unified string, with an optional user-defined delimiter separating each component part.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-textmerger"}, :properties {:insertPostTag {:default " ", :description "The tag indicates the end of the merged text. By default, the tag is an empty space.", :type "string"}, :insertPreTag {:default " ", :description "The tag indicates the start of the merged text. By default, the tag is an empty space.", :type "string"}}, :x-ms-discriminator-value "#Microsoft.Skills.Text.MergeSkill"}, :Index {:description "Represents a search index definition, which describes the fields and search behavior of an index.", :properties {:tokenizers {:description "The tokenizers for the index.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :items #:reference{:definitions "Tokenizer"}, :type "array"}, :scoringProfiles {:description "The scoring profiles for the index.", :items #:reference{:definitions "ScoringProfile"}, :type "array"}, :analyzers {:description "The analyzers for the index.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :items #:reference{:definitions "Analyzer"}, :type "array"}, :name {:description "The name of the index.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Naming-rules"}, :type "string"}, :fields {:description "The fields of the index.", :items #:reference{:definitions "Field"}, :type "array"}, :suggesters {:description "The suggesters for the index.", :items #:reference{:definitions "Suggester"}, :type "array"}, "@odata.etag" {:description "The ETag of the index.", :type "string", :x-ms-client-name "ETag"}, :charFilters {:description "The character filters for the index.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :items #:reference{:definitions "CharFilter"}, :type "array"}, :defaultScoringProfile {:description "The name of the scoring profile to use if none is specified in the query. If this property is not set and no scoring profile is specified in the query, then default scoring (tf-idf) will be used.", :type "string"}, :tokenFilters {:description "The token filters for the index.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :items #:reference{:definitions "TokenFilter"}, :type "array"}, :corsOptions #:reference{:definitions "CorsOptions"}}, :required ["name" "fields"]}, :MappingCharFilter {:allOf [#:reference{:definitions "CharFilter"}], :description "A character filter that applies mappings defined with the mappings option. Matching is greedy (longest pattern matching at a given point wins). Replacement is allowed to be the empty string. This character filter is implemented using Apache Lucene.", :externalDocs {:url "https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/charfilter/MappingCharFilter.html"}, :properties {:mappings {:description "A list of mappings of the following format: \"a=>b\" (all occurrences of the character \"a\" will be replaced with character \"b\").", :items {:type "string"}, :type "array"}}, :required ["mappings"], :x-ms-discriminator-value "#Microsoft.Azure.Search.MappingCharFilter"}, :ServiceLimits {:description "Represents various service level limits.", :properties {:maxComplexCollectionFieldsPerIndex {:description "The maximum number of fields of type Collection(Edm.ComplexType) allowed in an index.", :format "int32", :type "integer", :x-nullable true}, :maxComplexObjectsInCollectionsPerDocument {:description "The maximum number of objects in complex collections allowed per document.", :format "int32", :type "integer", :x-nullable true}, :maxFieldNestingDepthPerIndex {:description "The maximum depth which you can nest sub-fields in an index, including the top-level complex field. For example, a/b/c has a nesting depth of 3.", :format "int32", :type "integer", :x-nullable true}, :maxFieldsPerIndex {:description "The maximum allowed fields per index.", :format "int32", :type "integer", :x-nullable true}}}, :SplitSkillLanguage {:description "The language codes supported for input text by SplitSkill.", :enum ["da" "de" "en" "es" "fi" "fr" "it" "ko" "pt"], :type "string", :x-ms-enum {:modelAsString false, :name "SplitSkillLanguage"}}, :PathHierarchyTokenizerV2 {:allOf [#:reference{:definitions "Tokenizer"}], :description "Tokenizer for path-like hierarchies. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html"}, :properties {:delimiter {:default "/", :description "The delimiter character to use. Default is \"/\".", :format "char", :type "string"}, :maxTokenLength {:default 300, :description "The maximum token length. Default and maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :replacement {:default "/", :description "A value that, if set, replaces the delimiter character. Default is \"/\".", :format "char", :type "string"}, :reverse {:default false, :description "A value indicating whether to generate tokens in reverse order. Default is false.", :type "boolean", :x-ms-client-name "ReverseTokenOrder"}, :skip {:default 0, :description "The number of initial tokens to skip. Default is 0.", :format "int32", :type "integer", :x-ms-client-name "NumberOfTokensToSkip"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.PathHierarchyTokenizerV2"}, :IndexerStatus {:description "Represents the overall indexer status.", :enum ["unknown" "error" "running"], :type "string", :x-ms-enum {:name "IndexerStatus"}, :x-nullable false}, :ScoringFunctionInterpolation {:description "Defines the function used to interpolate score boosting across a range of documents.", :enum ["linear" "constant" "quadratic" "logarithmic"], :type "string", :x-ms-enum {:name "ScoringFunctionInterpolation"}}, :PatternTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html"}, :properties {:flags #:reference{:definitions "RegexFlags"}, :group {:default -1, :description "The zero-based ordinal of the matching group in the regular expression pattern to extract into tokens. Use -1 if you want to use the entire pattern to split the input into tokens, irrespective of matching groups. Default is -1.", :format "int32", :type "integer"}, :pattern {:default "\\W+", :description "A regular expression pattern to match token separators. Default is an expression that matches one or more whitespace characters.", :type "string"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.PatternTokenizer"}, :CharFilterName {:description "Defines the names of all character filters supported by Azure Cognitive Search.", :enum ["html_strip"], :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :type "string", :x-ms-enum {:modelAsString false, :name "CharFilterName"}}, :ScoringProfile {:description "Defines parameters for a search index that influence scoring in search queries.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Add-scoring-profiles-to-a-search-index"}, :properties {:functionAggregation #:reference{:definitions "ScoringFunctionAggregation"}, :functions {:description "The collection of functions that influence the scoring of documents.", :items #:reference{:definitions "ScoringFunction"}, :type "array"}, :name {:description "The name of the scoring profile.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Naming-rules"}, :type "string"}, :text #:reference{:definitions "TextWeights"}}, :required ["name"]}, :IndexingParameters {:description "Represents parameters for indexer execution.", :properties {:base64EncodeKeys {:default false, :description "Whether indexer will base64-encode all values that are inserted into key field of the target index. This is needed if keys can contain characters that are invalid in keys (such as dot '.'). Default is false.", :type "boolean"}, :batchSize {:description "The number of items that are read from the data source and indexed as a single batch in order to improve performance. The default depends on the data source type.", :format "int32", :type "integer"}, :configuration {:additionalProperties {:type "object"}, :description "A dictionary of indexer-specific configuration properties. Each name is the name of a specific property. Each value must be of a primitive type.", :type "object"}, :maxFailedItems {:default 0, :description "The maximum number of items that can fail indexing for indexer execution to still be considered successful. -1 means no limit. Default is 0.", :format "int32", :type "integer"}, :maxFailedItemsPerBatch {:default 0, :description "The maximum number of items in a single batch that can fail indexing for the batch to still be considered successful. -1 means no limit. Default is 0.", :format "int32", :type "integer"}}, :x-ms-external true}, :TextTranslationSkillLanguage {:description "The language codes supported for input text by TextTranslationSkill.", :enum ["af" "ar" "bn" "bs" "bg" "yue" "ca" "zh-Hans" "zh-Hant" "hr" "cs" "da" "nl" "en" "et" "fj" "fil" "fi" "fr" "de" "el" "ht" "he" "hi" "mww" "hu" "is" "id" "it" "ja" "sw" "tlh" "ko" "lv" "lt" "mg" "ms" "mt" "nb" "fa" "pl" "pt" "otq" "ro" "ru" "sm" "sr-Cyrl" "sr-Latn" "sk" "sl" "es" "sv" "ty" "ta" "te" "th" "to" "tr" "uk" "ur" "vi" "cy" "yua"], :type "string", :x-ms-enum {:modelAsString false, :name "TextTranslationSkillLanguage"}}, :PatternReplaceCharFilter {:allOf [#:reference{:definitions "CharFilter"}], :description "A character filter that replaces characters in the input string. It uses a regular expression to identify character sequences to preserve and a replacement pattern to identify characters to replace. For example, given the input text \"aa bb aa bb\", pattern \"(aa)\\s+(bb)\", and replacement \"$1#$2\", the result would be \"aa#bb aa#bb\". This character filter is implemented using Apache Lucene.", :externalDocs {:url "https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.html"}, :properties {:pattern {:description "A regular expression pattern.", :type "string"}, :replacement {:description "The replacement text.", :type "string"}}, :required ["pattern" "replacement"], :x-ms-discriminator-value "#Microsoft.Azure.Search.PatternReplaceCharFilter"}, :NamedEntityCategory {:description "A string indicating which named entity categories to return.", :enum ["location" "organization" "person"], :type "string", :x-ms-external true}, :MicrosoftLanguageTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Divides text using language-specific rules.", :properties {:isSearchTokenizer {:default false, :description "A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. Default is false.", :type "boolean"}, :language #:reference{:definitions "MicrosoftTokenizerLanguage"}, :maxTokenLength {:default 255, :description "The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300 characters. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the max token length set. Default is 255.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.MicrosoftLanguageTokenizer"}, :AnalyzeResult {:description "The result of testing an analyzer on text.", :properties {:tokens {:description "The list of tokens returned by the analyzer specified in the request.", :items #:reference{:definitions "TokenInfo"}, :type "array"}}}, :FieldMapping {:description "Defines a mapping between a field in a data source and a target field in an index.", :externalDocs {:url "https://docs.microsoft.com/azure/search/search-indexer-field-mappings"}, :properties {:mappingFunction #:reference{:definitions "FieldMappingFunction"}, :sourceFieldName {:description "The name of the field in the data source.", :type "string"}, :targetFieldName {:description "The name of the target field in the index. Same as the source field name by default.", :type "string"}}, :required ["sourceFieldName"]}, :SynonymMapListResult {:description "Response from a List SynonymMaps request. If successful, it includes the full definitions of all synonym maps.", :properties {:value {:description "The synonym maps in the Search service.", :items #:reference{:definitions "SynonymMap"}, :readOnly true, :type "array", :x-ms-client-name "SynonymMaps"}}}, :AnalyzerName {:description "Defines the names of all text analyzers supported by Azure Cognitive Search.", :enum ["ar.microsoft" "ar.lucene" "hy.lucene" "bn.microsoft" "eu.lucene" "bg.microsoft" "bg.lucene" "ca.microsoft" "ca.lucene" "zh-Hans.microsoft" "zh-Hans.lucene" "zh-Hant.microsoft" "zh-Hant.lucene" "hr.microsoft" "cs.microsoft" "cs.lucene" "da.microsoft" "da.lucene" "nl.microsoft" "nl.lucene" "en.microsoft" "en.lucene" "et.microsoft" "fi.microsoft" "fi.lucene" "fr.microsoft" "fr.lucene" "gl.lucene" "de.microsoft" "de.lucene" "el.microsoft" "el.lucene" "gu.microsoft" "he.microsoft" "hi.microsoft" "hi.lucene" "hu.microsoft" "hu.lucene" "is.microsoft" "id.microsoft" "id.lucene" "ga.lucene" "it.microsoft" "it.lucene" "ja.microsoft" "ja.lucene" "kn.microsoft" "ko.microsoft" "ko.lucene" "lv.microsoft" "lv.lucene" "lt.microsoft" "ml.microsoft" "ms.microsoft" "mr.microsoft" "nb.microsoft" "no.lucene" "fa.lucene" "pl.microsoft" "pl.lucene" "pt-BR.microsoft" "pt-BR.lucene" "pt-PT.microsoft" "pt-PT.lucene" "pa.microsoft" "ro.microsoft" "ro.lucene" "ru.microsoft" "ru.lucene" "sr-cyrillic.microsoft" "sr-latin.microsoft" "sk.microsoft" "sl.microsoft" "es.microsoft" "es.lucene" "sv.microsoft" "sv.lucene" "ta.microsoft" "te.microsoft" "th.microsoft" "th.lucene" "tr.microsoft" "tr.lucene" "uk.microsoft" "ur.microsoft" "vi.microsoft" "standard.lucene" "standardasciifolding.lucene" "keyword" "pattern" "simple" "stop" "whitespace"], :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Language-support"}, :type "string", :x-ms-enum {:modelAsString false, :name "AnalyzerName"}}, :AnalyzeRequest {:description "Specifies some text and analysis components used to break that text into tokens.", :properties {:analyzer #:reference{:definitions "AnalyzerName"}, :charFilters {:description "An optional list of character filters to use when breaking the given text. This parameter can only be set when using the tokenizer parameter.", :items #:reference{:definitions "CharFilterName"}, :type "array"}, :text {:description "The text to break into tokens.", :type "string"}, :tokenFilters {:description "An optional list of token filters to use when breaking the given text. This parameter can only be set when using the tokenizer parameter.", :items #:reference{:definitions "TokenFilterName"}, :type "array"}, :tokenizer #:reference{:definitions "TokenizerName"}}, :required ["text"]}, :DataSourceListResult {:description "Response from a List Datasources request. If successful, it includes the full definitions of all datasources.", :properties {:value {:description "The datasources in the Search service.", :items #:reference{:definitions "DataSource"}, :readOnly true, :type "array", :x-ms-client-name "DataSources"}}}, :PhoneticTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Create tokens for phonetic matches. This token filter is implemented using Apache Lucene.", :externalDocs {:url "https://lucene.apache.org/core/4_10_3/analyzers-phonetic/org/apache/lucene/analysis/phonetic/package-tree.html"}, :properties {:encoder #:reference{:definitions "PhoneticEncoder"}, :replace {:default true, :description "A value indicating whether encoded tokens should replace original tokens. If false, encoded tokens are added as synonyms. Default is true.", :type "boolean", :x-ms-client-name "ReplaceOriginalTokens"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.PhoneticTokenFilter"}, :PhoneticEncoder {:description "Identifies the type of phonetic encoder to use with a PhoneticTokenFilter.", :enum ["metaphone" "doubleMetaphone" "soundex" "refinedSoundex" "caverphone1" "caverphone2" "cologne" "nysiis" "koelnerPhonetik" "haasePhonetik" "beiderMorse"], :type "string", :x-ms-enum {:modelAsString false, :name "PhoneticEncoder"}}, :KeywordTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html"}, :properties {:bufferSize {:default 256, :description "The read buffer size in bytes. Default is 256.", :format "int32", :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.KeywordTokenizer", :x-ms-external true}, :StandardTokenizerV2 {:allOf [#:reference{:definitions "Tokenizer"}], :description "Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html"}, :properties {:maxTokenLength {:default 255, :description "The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.StandardTokenizerV2"}, :WebApiHttpHeaders {:additionalProperties {:type "string"}, :description "A dictionary of http request headers.", :type "object"}, :DataChangeDetectionPolicy {:description "Abstract base class for data change detection policies.", :discriminator "@odata.type", :properties {"@odata.type" {:type "string"}}, :required ["@odata.type"]}, :ClassicTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Grammar-based tokenizer that is suitable for processing most European-language documents. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html"}, :properties {:maxTokenLength {:default 255, :description "The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.ClassicTokenizer"}, :SoftDeleteColumnDeletionDetectionPolicy {:allOf [#:reference{:definitions "DataDeletionDetectionPolicy"}], :description "Defines a data deletion detection policy that implements a soft-deletion strategy. It determines whether an item should be deleted based on the value of a designated 'soft delete' column.", :properties {:softDeleteColumnName {:description "The name of the column to use for soft-deletion detection.", :type "string"}, :softDeleteMarkerValue {:description "The marker value that identifies an item as deleted.", :type "string"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy"}, :EntityRecognitionSkillLanguage {:description "The language codes supported for input text by EntityRecognitionSkill.", :enum ["de" "en" "es" "fr" "it"], :type "string", :x-ms-enum {:modelAsString false, :name "EntityRecognitionSkillLanguage"}}, :ElisionTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Removes elisions. For example, \"l'avion\" (the plane) will be converted to \"avion\" (plane). This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html"}, :properties {:articles {:description "The set of articles to remove.", :items {:type "string"}, :type "array"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.ElisionTokenFilter"}, :DefaultCognitiveServices {:allOf [#:reference{:definitions "CognitiveServices"}], :description "An empty object that represents the default cognitive service resource for a skillset.", :x-ms-discriminator-value "#Microsoft.Azure.Search.DefaultCognitiveServices"}, :CorsOptions {:description "Defines options to control Cross-Origin Resource Sharing (CORS) for an index.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Create-Index"}, :properties {:allowedOrigins {:description "The list of origins from which JavaScript code will be granted access to your index. Can contain a list of hosts of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a single '*' to allow all origins (not recommended).", :items {:type "string"}, :type "array"}, :maxAgeInSeconds {:description "The duration for which browsers should cache CORS preflight responses. Defaults to 5 minutes.", :format "int64", :type "integer"}}, :required ["allowedOrigins"]}, :TokenizerName {:description "Defines the names of all tokenizers supported by Azure Cognitive Search.", :enum ["classic" "edgeNGram" "keyword_v2" "letter" "lowercase" "microsoft_language_tokenizer" "microsoft_language_stemming_tokenizer" "nGram" "path_hierarchy_v2" "pattern" "standard_v2" "uax_url_email" "whitespace"], :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :type "string", :x-ms-enum {:modelAsString false, :name "TokenizerName"}}, :EntityCategory {:description "A string indicating what entity categories to return.", :enum ["location" "organization" "person" "quantity" "datetime" "url" "email"], :type "string", :x-ms-enum {:modelAsString false, :name "EntityCategory"}}, :StandardAnalyzer {:allOf [#:reference{:definitions "Analyzer"}], :description "Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop filter.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardAnalyzer.html"}, :properties {:maxTokenLength {:default 255, :description "The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.", :format "int32", :maximum 300, :type "integer"}, :stopwords {:description "A list of stopwords.", :items {:type "string"}, :type "array"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.StandardAnalyzer"}, :ShaperSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill for reshaping the outputs. It creates a complex type to support composite fields (also known as multipart fields).", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-shaper"}, :x-ms-discriminator-value "#Microsoft.Skills.Util.ShaperSkill"}, :TruncateTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Truncates the terms to a specific length. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html"}, :properties {:length {:default 300, :description "The length at which terms will be truncated. Default and maximum is 300.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.TruncateTokenFilter"}, :NGramTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html"}, :properties {:maxGram {:default 2, :description "The maximum n-gram length. Default is 2.", :format "int32", :type "integer"}, :minGram {:default 1, :description "The minimum n-gram length. Default is 1. Must be less than the value of maxGram.", :format "int32", :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.NGramTokenFilter", :x-ms-external true}, :SqlIntegratedChangeTrackingPolicy {:allOf [#:reference{:definitions "DataChangeDetectionPolicy"}], :description "Defines a data change detection policy that captures changes using the Integrated Change Tracking feature of Azure SQL Database.", :x-ms-discriminator-value "#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy"}, :SynonymTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Matches single or multi-word synonyms in a token stream. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/synonym/SynonymFilter.html"}, :properties {:expand {:default true, :description "A value indicating whether all words in the list of synonyms (if => notation is not used) will map to one another. If true, all words in the list of synonyms (if => notation is not used) will map to one another. The following list: incredible, unbelievable, fabulous, amazing is equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable, fabulous, amazing. If false, the following list: incredible, unbelievable, fabulous, amazing will be equivalent to: incredible, unbelievable, fabulous, amazing => incredible. Default is true.", :type "boolean"}, :ignoreCase {:default false, :description "A value indicating whether to case-fold input for matching. Default is false.", :type "boolean"}, :synonyms {:description "A list of synonyms in following one of two formats: 1. incredible, unbelievable, fabulous => amazing - all terms on the left side of => symbol will be replaced with all terms on its right side; 2. incredible, unbelievable, fabulous, amazing - comma separated list of equivalent words. Set the expand option to change how this list is interpreted.", :items {:type "string"}, :type "array"}}, :required ["synonyms"], :x-ms-discriminator-value "#Microsoft.Azure.Search.SynonymTokenFilter"}, :CjkBigramTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Forms bigrams of CJK terms that are generated from StandardTokenizer. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html"}, :properties {:ignoreScripts {:description "The scripts to ignore.", :items #:reference{:definitions "CjkBigramTokenFilterScripts"}, :type "array"}, :outputUnigrams {:default false, :description "A value indicating whether to output both unigrams and bigrams (if true), or just bigrams (if false). Default is false.", :type "boolean"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.CjkBigramTokenFilter"}, :OcrSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill that extracts text from image files.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-ocr"}, :properties {:defaultLanguageCode #:reference{:definitions "OcrSkillLanguage"}, :detectOrientation {:default false, :description "A value indicating to turn orientation detection on or not. Default is false.", :type "boolean", :x-ms-client-name "ShouldDetectOrientation"}, :textExtractionAlgorithm #:reference{:definitions "TextExtractionAlgorithm"}}, :x-ms-discriminator-value "#Microsoft.Skills.Vision.OcrSkill"}, :ItemError {:description "Represents an item- or document-level indexing error.", :properties {:details {:description "Additional, verbose details about the error to assist in debugging the indexer. This may not be always available.", :readOnly true, :type "string"}, :documentationLink {:description "A link to a troubleshooting guide for these classes of errors. This may not be always available.", :readOnly true, :type "string"}, :errorMessage {:description "The message describing the error that occurred while processing the item.", :readOnly true, :type "string"}, :key {:description "The key of the item for which indexing failed.", :readOnly true, :type "string"}, :name {:description "The name of the source at which the error originated. For example, this could refer to a particular skill in the attached skillset. This may not be always available.", :readOnly true, :type "string"}, :statusCode {:description "The status code indicating why the indexing operation failed. Possible values include: 400 for a malformed input document, 404 for document not found, 409 for a version conflict, 422 when the index is temporarily unavailable, or 503 for when the service is too busy.", :format "int32", :readOnly true, :type "integer", :x-nullable false}}}, :DataSourceType {:description "Defines the type of a datasource.", :enum ["azuresql" "cosmosdb" "azureblob" "azuretable"], :type "string", :x-ms-enum {:modelAsString false, :name "DataSourceType"}}, :MagnitudeScoringFunction {:allOf [#:reference{:definitions "ScoringFunction"}], :description "Defines a function that boosts scores based on the magnitude of a numeric field.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Add-scoring-profiles-to-a-search-index"}, :properties {:magnitude #:reference{:definitions "MagnitudeScoringParameters"}}, :required ["magnitude"], :x-ms-discriminator-value "magnitude"}, :UaxUrlEmailTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Tokenizes urls and emails as one token. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html"}, :properties {:maxTokenLength {:default 255, :description "The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.UaxUrlEmailTokenizer"}, :CognitiveServicesByKey {:allOf [#:reference{:definitions "CognitiveServices"}], :description "A cognitive service resource provisioned with a key that is attached to a skillset.", :properties {:key {:type "string"}}, :required ["key"], :x-ms-discriminator-value "#Microsoft.Azure.Search.CognitiveServicesByKey"}, :ResourceCounter {:description "Represents a resource's usage and quota.", :properties {:quota {:description "The resource amount quota.", :format "int64", :type "integer", :x-nullable true}, :usage {:description "The resource usage amount.", :format "int64", :type "integer", :x-nullable false}}}, :EdgeNGramTokenFilterSide {:description "Specifies which side of the input an n-gram should be generated from.", :enum ["front" "back"], :type "string", :x-ms-enum {:modelAsString false, :name "EdgeNGramTokenFilterSide"}}, :SnowballTokenFilterLanguage {:description "The language to use for a Snowball token filter.", :enum ["armenian" "basque" "catalan" "danish" "dutch" "english" "finnish" "french" "german" "german2" "hungarian" "italian" "kp" "lovins" "norwegian" "porter" "portuguese" "romanian" "russian" "spanish" "swedish" "turkish"], :type "string", :x-ms-enum {:modelAsString false, :name "SnowballTokenFilterLanguage"}}, :KeyPhraseExtractionSkillLanguage {:description "The language codes supported for input text by KeyPhraseExtractionSkill.", :enum ["da" "nl" "en" "fi" "fr" "de" "it" "ja" "ko" "no" "pl" "pt-PT" "pt-BR" "ru" "es" "sv"], :type "string", :x-ms-enum {:modelAsString false, :name "KeyPhraseExtractionSkillLanguage"}}, :DistanceScoringFunction {:allOf [#:reference{:definitions "ScoringFunction"}], :description "Defines a function that boosts scores based on distance from a geographic location.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Add-scoring-profiles-to-a-search-index"}, :properties {:distance #:reference{:definitions "DistanceScoringParameters"}}, :required ["distance"], :x-ms-discriminator-value "distance"}, :TextExtractionAlgorithm {:description "A value indicating which algorithm to use. Default is printed.", :enum ["printed" "handwritten"], :type "string", :x-ms-enum {:modelAsString false, :name "TextExtractionAlgorithm"}}, :MicrosoftStemmingTokenizerLanguage {:description "Lists the languages supported by the Microsoft language stemming tokenizer.", :enum ["arabic" "bangla" "bulgarian" "catalan" "croatian" "czech" "danish" "dutch" "english" "estonian" "finnish" "french" "german" "greek" "gujarati" "hebrew" "hindi" "hungarian" "icelandic" "indonesian" "italian" "kannada" "latvian" "lithuanian" "malay" "malayalam" "marathi" "norwegianBokmaal" "polish" "portuguese" "portugueseBrazilian" "punjabi" "romanian" "russian" "serbianCyrillic" "serbianLatin" "slovak" "slovenian" "spanish" "swedish" "tamil" "telugu" "turkish" "ukrainian" "urdu"], :type "string", :x-ms-enum {:modelAsString false, :name "MicrosoftStemmingTokenizerLanguage"}}, :StemmerOverrideTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Provides the ability to override other stemming filters with custom dictionary-based stemming. Any dictionary-stemmed terms will be marked as keywords so that they will not be stemmed with stemmers down the chain. Must be placed before any stemming filters. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.html"}, :properties {:rules {:description "A list of stemming rules in the following format: \"word => stem\", for example: \"ran => run\".", :items {:type "string"}, :type "array"}}, :required ["rules"], :x-ms-discriminator-value "#Microsoft.Azure.Search.StemmerOverrideTokenFilter"}, :PatternCaptureTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Uses Java regexes to emit multiple tokens - one for each capture group in one or more patterns. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternCaptureGroupTokenFilter.html"}, :properties {:patterns {:description "A list of patterns to match against each token.", :items {:type "string"}, :type "array"}, :preserveOriginal {:default true, :description "A value indicating whether to return the original token even if one of the patterns matches. Default is true.", :type "boolean"}}, :required ["patterns"], :x-ms-discriminator-value "#Microsoft.Azure.Search.PatternCaptureTokenFilter"}, :NGramTokenFilterV2 {:allOf [#:reference{:definitions "TokenFilter"}], :description "Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html"}, :properties {:maxGram {:default 2, :description "The maximum n-gram length. Default is 2. Maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :minGram {:default 1, :description "The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.", :format "int32", :maximum 300, :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.NGramTokenFilterV2"}, :TokenFilterName {:description "Defines the names of all token filters supported by Azure Cognitive Search.", :enum ["arabic_normalization" "apostrophe" "asciifolding" "cjk_bigram" "cjk_width" "classic" "common_grams" "edgeNGram_v2" "elision" "german_normalization" "hindi_normalization" "indic_normalization" "keyword_repeat" "kstem" "length" "limit" "lowercase" "nGram_v2" "persian_normalization" "phonetic" "porter_stem" "reverse" "scandinavian_normalization" "scandinavian_folding" "shingle" "snowball" "sorani_normalization" "stemmer" "stopwords" "trim" "truncate" "unique" "uppercase" "word_delimiter"], :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :type "string", :x-ms-enum {:modelAsString false, :name "TokenFilterName"}}, :Tokenizer {:description "Abstract base class for tokenizers.", :discriminator "@odata.type", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :properties {"@odata.type" {:type "string"}, :name {:description "The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/custom-analyzers-in-azure-search#index-attribute-reference"}, :type "string"}}, :required ["@odata.type" "name"]}, :ServiceStatistics {:description "Response from a get service statistics request. If successful, it includes service level counters and limits.", :properties {:counters #:reference{:definitions "ServiceCounters"}, :limits #:reference{:definitions "ServiceLimits"}}}, :EdgeNGramTokenFilterV2 {:allOf [#:reference{:definitions "TokenFilter"}], :description "Generates n-grams of the given size(s) starting from the front or the back of an input token. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html"}, :properties {:maxGram {:default 2, :description "The maximum n-gram length. Default is 2. Maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :minGram {:default 1, :description "The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.", :format "int32", :maximum 300, :type "integer"}, :side #:reference{:definitions "EdgeNGramTokenFilterSide"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.EdgeNGramTokenFilterV2"}, :TokenInfo {:description "Information about a token returned by an analyzer.", :properties {:endOffset {:description "The index of the last character of the token in the input text.", :format "int32", :readOnly true, :type "integer"}, :position {:description "The position of the token in the input text relative to other tokens. The first token in the input text has position 0, the next has position 1, and so on. Depending on the analyzer used, some tokens might have the same position, for example if they are synonyms of each other.", :format "int32", :readOnly true, :type "integer"}, :startOffset {:description "The index of the first character of the token in the input text.", :format "int32", :readOnly true, :type "integer"}, :token {:description "The token returned by the analyzer.", :readOnly true, :type "string"}}}, :StopwordsList {:description "Identifies a predefined list of language-specific stopwords.", :enum ["arabic" "armenian" "basque" "brazilian" "bulgarian" "catalan" "czech" "danish" "dutch" "english" "finnish" "french" "galician" "german" "greek" "hindi" "hungarian" "indonesian" "irish" "italian" "latvian" "norwegian" "persian" "portuguese" "romanian" "russian" "sorani" "spanish" "swedish" "thai" "turkish"], :type "string", :x-ms-enum {:modelAsString false, :name "StopwordsList"}}, :CharFilter {:description "Abstract base class for character filters.", :discriminator "@odata.type", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :properties {"@odata.type" {:type "string"}, :name {:description "The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/custom-analyzers-in-azure-search#index-attribute-reference"}, :type "string"}}, :required ["@odata.type" "name"]}, :ItemWarning {:description "Represents an item-level warning.", :properties {:details {:description "Additional, verbose details about the warning to assist in debugging the indexer. This may not be always available.", :readOnly true, :type "string"}, :documentationLink {:description "A link to a troubleshooting guide for these classes of warnings. This may not be always available.", :readOnly true, :type "string"}, :key {:description "The key of the item which generated a warning.", :readOnly true, :type "string"}, :message {:description "The message describing the warning that occurred while processing the item.", :readOnly true, :type "string"}, :name {:description "The name of the source at which the warning originated. For example, this could refer to a particular skill in the attached skillset. This may not be always available.", :readOnly true, :type "string"}}}, :SnowballTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "A filter that stems words using a Snowball-generated stemmer. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/snowball/SnowballFilter.html"}, :properties {:language #:reference{:definitions "SnowballTokenFilterLanguage"}}, :required ["language"], :x-ms-discriminator-value "#Microsoft.Azure.Search.SnowballTokenFilter"}, :IndexerExecutionInfo {:description "Represents the current status and execution history of an indexer.", :properties {:executionHistory {:description "History of the recent indexer executions, sorted in reverse chronological order.", :items #:reference{:definitions "IndexerExecutionResult"}, :readOnly true, :type "array"}, :lastResult #:reference{:definitions "IndexerExecutionResult"}, :limits #:reference{:definitions "IndexerLimits"}, :status #:reference{:definitions "IndexerStatus"}}}, :RegexFlags {:description "Defines flags that can be combined to control how regular expressions are used in the pattern analyzer and pattern tokenizer.", :enum ["CANON_EQ" "CASE_INSENSITIVE" "COMMENTS" "DOTALL" "LITERAL" "MULTILINE" "UNICODE_CASE" "UNIX_LINES"], :externalDocs {:url "http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html#field_summary"}, :type "string", :x-ms-enum {:modelAsString false, :name "RegexFlags"}}, :TokenFilter {:description "Abstract base class for token filters.", :discriminator "@odata.type", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search"}, :properties {"@odata.type" {:type "string"}, :name {:description "The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/custom-analyzers-in-azure-search#index-attribute-reference"}, :type "string"}}, :required ["@odata.type" "name"]}, :PatternAnalyzer {:allOf [#:reference{:definitions "Analyzer"}], :description "Flexibly separates text into terms via a regular expression pattern. This analyzer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.html"}, :properties {:flags #:reference{:definitions "RegexFlags"}, :lowercase {:default true, :description "A value indicating whether terms should be lower-cased. Default is true.", :type "boolean", :x-ms-client-name "LowerCaseTerms"}, :pattern {:default "\\W+", :description "A regular expression pattern to match token separators. Default is an expression that matches one or more whitespace characters.", :type "string"}, :stopwords {:description "A list of stopwords.", :items {:type "string"}, :type "array"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.PatternAnalyzer"}, :ImageDetail {:description "A string indicating which domain-specific details to return.", :enum ["celebrities" "landmarks"], :type "string", :x-ms-enum {:modelAsString false, :name "ImageDetail"}}, :IndexerListResult {:description "Response from a List Indexers request. If successful, it includes the full definitions of all indexers.", :properties {:value {:description "The indexers in the Search service.", :items #:reference{:definitions "Indexer"}, :readOnly true, :type "array", :x-ms-client-name "Indexers"}}}, :VisualFeature {:description "The strings indicating what visual feature types to return.", :enum ["categories" "tags" "description" "faces" "imageType" "color"], :type "string", :x-ms-enum {:modelAsString false, :name "VisualFeature"}}, :MicrosoftTokenizerLanguage {:description "Lists the languages supported by the Microsoft language tokenizer.", :enum ["bangla" "bulgarian" "catalan" "chineseSimplified" "chineseTraditional" "croatian" "czech" "danish" "dutch" "english" "french" "german" "greek" "gujarati" "hindi" "icelandic" "indonesian" "italian" "japanese" "kannada" "korean" "malay" "malayalam" "marathi" "norwegianBokmaal" "polish" "portuguese" "portugueseBrazilian" "punjabi" "romanian" "russian" "serbianCyrillic" "serbianLatin" "slovenian" "spanish" "swedish" "tamil" "telugu" "thai" "ukrainian" "urdu" "vietnamese"], :type "string", :x-ms-enum {:modelAsString false, :name "MicrosoftTokenizerLanguage"}}, :KeywordMarkerTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Marks terms as keywords. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeywordMarkerFilter.html"}, :properties {:ignoreCase {:default false, :description "A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.", :type "boolean"}, :keywords {:description "A list of words to mark as keywords.", :items {:type "string"}, :type "array"}}, :required ["keywords"], :x-ms-discriminator-value "#Microsoft.Azure.Search.KeywordMarkerTokenFilter"}, :SkillsetListResult {:description "Response from a list Skillset request. If successful, it includes the full definitions of all skillsets.", :properties {:value {:description "The skillsets defined in the Search service.", :items #:reference{:definitions "Skillset"}, :readOnly true, :type "array", :x-ms-client-name "Skillsets"}}}, :IndexGetStatisticsResult {:description "Statistics for a given index. Statistics are collected periodically and are not guaranteed to always be up-to-date.", :properties {:documentCount {:description "The number of documents in the index.", :format "int64", :readOnly true, :type "integer", :x-nullable false}, :storageSize {:description "The amount of storage in bytes consumed by the index.", :format "int64", :readOnly true, :type "integer", :x-nullable false}}}, :InputFieldMappingEntry {:description "Input field mapping for a skill.", :properties {:inputs {:description "The recursive inputs used when creating a complex type.", :items #:reference{:definitions "InputFieldMappingEntry"}, :type "array"}, :name {:description "The name of the input.", :type "string"}, :source {:description "The source of the input.", :type "string"}, :sourceContext {:description "The source context used for selecting recursive inputs.", :type "string"}}, :required ["name"]}, :ImageAnalysisSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill that analyzes image files. It extracts a rich set of visual features based on the image content.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-image-analysis"}, :properties {:defaultLanguageCode #:reference{:definitions "ImageAnalysisSkillLanguage"}, :details {:description "A string indicating which domain-specific details to return.", :items #:reference{:definitions "ImageDetail"}, :type "array"}, :visualFeatures {:description "A list of visual features.", :items #:reference{:definitions "VisualFeature"}, :type "array"}}, :x-ms-discriminator-value "#Microsoft.Skills.Vision.ImageAnalysisSkill"}, :MagnitudeScoringParameters {:description "Provides parameter values to a magnitude scoring function.", :properties {:boostingRangeEnd {:description "The field value at which boosting ends.", :format "double", :type "number"}, :boostingRangeStart {:description "The field value at which boosting starts.", :format "double", :type "number"}, :constantBoostBeyondRange {:description "A value indicating whether to apply a constant boost for field values beyond the range end value; default is false.", :type "boolean", :x-ms-client-name "ShouldBoostBeyondRangeByConstant"}}, :required ["boostingRangeStart" "boostingRangeEnd"]}, :StopAnalyzer {:allOf [#:reference{:definitions "Analyzer"}], :description "Divides text at non-letters; Applies the lowercase and stopword token filters. This analyzer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/StopAnalyzer.html"}, :properties {:stopwords {:description "A list of stopwords.", :items {:type "string"}, :type "array"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.StopAnalyzer"}, :KeepTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "A token filter that only keeps tokens with text contained in a specified list of words. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeepWordFilter.html"}, :properties {:keepWords {:description "The list of words to keep.", :items {:type "string"}, :type "array"}, :keepWordsCase {:default false, :description "A value indicating whether to lower case all words first. Default is false.", :type "boolean", :x-ms-client-name "LowerCaseKeepWords"}}, :required ["keepWords"], :x-ms-discriminator-value "#Microsoft.Azure.Search.KeepTokenFilter"}, :NGramTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html"}, :properties {:maxGram {:default 2, :description "The maximum n-gram length. Default is 2. Maximum is 300.", :format "int32", :maximum 300, :type "integer"}, :minGram {:default 1, :description "The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.", :format "int32", :maximum 300, :type "integer"}, :tokenChars {:description "Character classes to keep in the tokens.", :items #:reference{:definitions "TokenCharacterKind"}, :type "array"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.NGramTokenizer"}, :DataDeletionDetectionPolicy {:description "Abstract base class for data deletion detection policies.", :discriminator "@odata.type", :properties {"@odata.type" {:type "string"}}, :required ["@odata.type"]}, :TextTranslationSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill to translate text from one language to another.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-text-translation"}, :properties {:defaultFromLanguageCode #:reference{:definitions "TextTranslationSkillLanguage"}, :defaultToLanguageCode #:reference{:definitions "TextTranslationSkillLanguage"}, :suggestedFrom #:reference{:definitions "TextTranslationSkillLanguage"}}, :required ["defaultToLanguageCode"], :x-ms-discriminator-value "#Microsoft.Skills.Text.TranslationSkill"}, :StandardTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html"}, :properties {:maxTokenLength {:default 255, :description "The maximum token length. Default is 255. Tokens longer than the maximum length are split.", :format "int32", :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.StandardTokenizer", :x-ms-external true}, :DataSource {:description "Represents a datasource definition, which can be used to configure an indexer.", :properties {"@odata.etag" {:description "The ETag of the DataSource.", :type "string", :x-ms-client-name "ETag"}, :container #:reference{:definitions "DataContainer"}, :credentials #:reference{:definitions "DataSourceCredentials"}, :dataChangeDetectionPolicy #:reference{:definitions "DataChangeDetectionPolicy"}, :dataDeletionDetectionPolicy #:reference{:definitions "DataDeletionDetectionPolicy"}, :description {:description "The description of the datasource.", :type "string"}, :name {:description "The name of the datasource.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Naming-rules"}, :type "string"}, :type #:reference{:definitions "DataSourceType"}}, :required ["name" "type" "credentials" "container"]}, :ScoringFunction {:description "Abstract base class for functions that can modify document scores during ranking.", :discriminator "type", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Add-scoring-profiles-to-a-search-index"}, :properties {:boost {:description "A multiplier for the raw score. Must be a positive number not equal to 1.0.", :format "double", :type "number"}, :fieldName {:description "The name of the field used as input to the scoring function.", :type "string"}, :interpolation #:reference{:definitions "ScoringFunctionInterpolation"}, :type {:type "string"}}, :required ["type" "fieldName" "boost"]}, :EntityRecognitionSkill {:allOf [#:reference{:definitions "Skill"}], :description "Text analytics entity recognition.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-entity-recognition"}, :properties {:categories {:description "A list of entity categories that should be extracted.", :items #:reference{:definitions "EntityCategory"}, :type "array"}, :defaultLanguageCode #:reference{:definitions "EntityRecognitionSkillLanguage"}, :includeTypelessEntities {:description "Determines whether or not to include entities which are well known but don't conform to a pre-defined type. If this configuration is not set (default), set to null or set to false, entities which don't conform to one of the pre-defined types will not be surfaced.", :type "boolean", :x-nullable true}, :minimumPrecision {:description "A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified. If not set (default), or if explicitly set to null, all entities will be included.", :format "double", :type "number", :x-nullable true}}, :x-ms-discriminator-value "#Microsoft.Skills.Text.EntityRecognitionSkill"}, :SentimentSkillLanguage {:description "The language codes supported for input text by SentimentSkill.", :enum ["da" "nl" "en" "fi" "fr" "de" "el" "it" "no" "pl" "pt-PT" "ru" "es" "sv" "tr"], :type "string", :x-ms-enum {:modelAsString false, :name "SentimentSkillLanguage"}}, :PatternReplaceTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "A character filter that replaces characters in the input string. It uses a regular expression to identify character sequences to preserve and a replacement pattern to identify characters to replace. For example, given the input text \"aa bb aa bb\", pattern \"(aa)\\s+(bb)\", and replacement \"$1#$2\", the result would be \"aa#bb aa#bb\". This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceFilter.html"}, :properties {:pattern {:description "A regular expression pattern.", :type "string"}, :replacement {:description "The replacement text.", :type "string"}}, :required ["pattern" "replacement"], :x-ms-discriminator-value "#Microsoft.Azure.Search.PatternReplaceTokenFilter"}, :TextWeights {:description "Defines weights on index fields for which matches should boost scoring in search queries.", :properties {:weights {:additionalProperties {:format "double", :type "number", :x-nullable false}, :description "The dictionary of per-field weights to boost document scoring. The keys are field names and the values are the weights for each field.", :type "object"}}, :required ["weights"]}, :ConditionalSkill {:allOf [#:reference{:definitions "Skill"}], :description "A skill that enables scenarios that require a Boolean operation to determine the data to assign to an output.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-conditional"}, :x-ms-discriminator-value "#Microsoft.Skills.Util.ConditionalSkill"}, :LimitTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Limits the number of tokens while indexing. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilter.html"}, :properties {:consumeAllTokens {:default false, :description "A value indicating whether all tokens from the input must be consumed even if maxTokenCount is reached. Default is false.", :type "boolean"}, :maxTokenCount {:default 1, :description "The maximum number of tokens to produce. Default is 1.", :format "int32", :type "integer"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.LimitTokenFilter"}, :FreshnessScoringParameters {:description "Provides parameter values to a freshness scoring function.", :properties {:boostingDuration {:description "The expiration period after which boosting will stop for a particular document.", :format "duration", :type "string"}}, :required ["boostingDuration"]}, :CjkBigramTokenFilterScripts {:description "Scripts that can be ignored by CjkBigramTokenFilter.", :enum ["han" "hiragana" "katakana" "hangul"], :type "string", :x-ms-enum {:modelAsString false, :name "CjkBigramTokenFilterScripts"}}, :UniqueTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Filters out tokens with same text as the previous token. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html"}, :properties {:onlyOnSamePosition {:default false, :description "A value indicating whether to remove duplicates only at the same position. Default is false.", :type "boolean"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.UniqueTokenFilter"}, :IndexingSchedule {:description "Represents a schedule for indexer execution.", :properties {:interval {:description "The interval of time between indexer executions.", :format "duration", :type "string"}, :startTime {:description "The time when an indexer should start running.", :format "date-time", :type "string"}}, :required ["interval"]}, :PathHierarchyTokenizer {:allOf [#:reference{:definitions "Tokenizer"}], :description "Tokenizer for path-like hierarchies. This tokenizer is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html"}, :properties {:bufferSize {:default 1024, :description "The buffer size. Default is 1024.", :format "int32", :type "integer"}, :delimiter {:default "/", :description "The delimiter character to use. Default is \"/\".", :format "char", :type "string"}, :replacement {:default "/", :description "A value that, if set, replaces the delimiter character. Default is \"/\".", :format "char", :type "string"}, :reverse {:default false, :description "A value indicating whether to generate tokens in reverse order. Default is false.", :type "boolean", :x-ms-client-name "ReverseTokenOrder"}, :skip {:default 0, :description "The number of initial tokens to skip. Default is 0.", :format "int32", :type "integer", :x-ms-client-name "NumberOfTokensToSkip"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.PathHierarchyTokenizer", :x-ms-external true}, :Analyzer {:description "Abstract base class for analyzers.", :discriminator "@odata.type", :properties {"@odata.type" {:type "string"}, :name {:description "The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/custom-analyzers-in-azure-search#index-attribute-reference"}, :type "string"}}, :required ["@odata.type" "name"]}, :StemmerTokenFilterLanguage {:description "The language to use for a stemmer token filter.", :enum ["arabic" "armenian" "basque" "brazilian" "bulgarian" "catalan" "czech" "danish" "dutch" "dutchKp" "english" "lightEnglish" "minimalEnglish" "possessiveEnglish" "porter2" "lovins" "finnish" "lightFinnish" "french" "lightFrench" "minimalFrench" "galician" "minimalGalician" "german" "german2" "lightGerman" "minimalGerman" "greek" "hindi" "hungarian" "lightHungarian" "indonesian" "irish" "italian" "lightItalian" "sorani" "latvian" "norwegian" "lightNorwegian" "minimalNorwegian" "lightNynorsk" "minimalNynorsk" "portuguese" "lightPortuguese" "minimalPortuguese" "portugueseRslp" "romanian" "russian" "lightRussian" "spanish" "lightSpanish" "swedish" "lightSwedish" "turkish"], :type "string", :x-ms-enum {:modelAsString false, :name "StemmerTokenFilterLanguage"}}, :IndexerLimits {:properties {:maxDocumentContentCharactersToExtract {:description "The maximum number of characters that will be extracted from a document picked up for indexing.", :format "int64", :readOnly true, :type "number"}, :maxDocumentExtractionSize {:description "The maximum size of a document, in bytes, which will be considered valid for indexing.", :format "int64", :readOnly true, :type "number"}, :maxRunTime {:description "The maximum duration that the indexer is permitted to run for one execution.", :format "duration", :readOnly true, :type "string"}}}, :StopwordsTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Removes stop words from a token stream. This token filter is implemented using Apache Lucene.", :externalDocs {:url "http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/StopFilter.html"}, :properties {:ignoreCase {:default false, :description "A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.", :type "boolean"}, :removeTrailing {:default true, :description "A value indicating whether to ignore the last search term if it's a stop word. Default is true.", :type "boolean", :x-ms-client-name "RemoveTrailingStopWords"}, :stopwords {:description "The list of stopwords. This property and the stopwords list property cannot both be set.", :items {:type "string"}, :type "array"}, :stopwordsList #:reference{:definitions "StopwordsList"}}, :x-ms-discriminator-value "#Microsoft.Azure.Search.StopwordsTokenFilter"}, :StemmerTokenFilter {:allOf [#:reference{:definitions "TokenFilter"}], :description "Language specific stemming filter. This token filter is implemented using Apache Lucene.", :externalDocs {:url "https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search#TokenFilters"}, :properties {:language #:reference{:definitions "StemmerTokenFilterLanguage"}}, :required ["language"], :x-ms-discriminator-value "#Microsoft.Azure.Search.StemmerTokenFilter"}, :DataType {:description "Defines the data type of a field in a search index.", :enum ["Edm.String" "Edm.Int32" "Edm.Int64" "Edm.Double" "Edm.Boolean" "Edm.DateTimeOffset" "Edm.GeographyPoint" "Edm.ComplexType"], :type "string", :x-ms-enum {:modelAsString false, :name "DataType"}}, :SentimentSkill {:allOf [#:reference{:definitions "Skill"}], :description "Text analytics positive-negative sentiment analysis, scored as a floating point value in a range of zero to 1.", :externalDocs {:url "https://docs.microsoft.com/azure/search/cognitive-search-skill-sentiment"}, :properties {:defaultLanguageCode #:reference{:definitions "SentimentSkillLanguage"}}, :x-ms-discriminator-value "#Microsoft.Skills.Text.SentimentSkill"}, :IndexerExecutionResult {:description "Represents the result of an individual indexer execution.", :properties {:errors {:description "The item-level indexing errors.", :items #:reference{:definitions "ItemError"}, :readOnly true, :type "array"}, :errorMessage {:description "The error message indicating the top-level error, if any.", :readOnly true, :type "string"}, :startTime {:description "The start time of this indexer execution.", :format "date-time", :readOnly true, :type "string"}, :endTime {:description "The end time of this indexer execution, if the execution has already completed.", :format "date-time", :readOnly true, :type "string"}, :initialTrackingState {:description "Change tracking state with which an indexer execution started.", :readOnly true, :type "string"}, :warnings {:description "The item-level indexing warnings.", :items #:reference{:definitions "ItemWarning"}, :readOnly true, :type "array"}, :itemsProcessed {:description "The number of items that were processed during this indexer execution. This includes both successfully processed items and items where indexing was attempted but failed.", :format "int32", :readOnly true, :type "integer", :x-ms-client-name "ItemCount", :x-nullable false}, :finalTrackingState {:description "Change tracking state with which an indexer execution finished.", :readOnly true, :type "string"}, :status #:reference{:definitions "IndexerExecutionStatus"}, :itemsFailed {:description "The number of items that failed to be indexed during this indexer execution.", :format "int32", :readOnly true, :type "integer", :x-ms-client-name "FailedItemCount", :x-nullable false}}}}}